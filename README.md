# SAE Helix mRNA

**ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Auto-Encoder) æŽ¢ç´¢ Helix-mRNA æ¨¡åž‹çš„ç”Ÿç‰©å­¦å¯è§£é‡Šæ€§**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ðŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [æ ¸å¿ƒåŽŸç†](#æ ¸å¿ƒåŽŸç†)
- [é¡¹ç›®ç»“æž„](#é¡¹ç›®ç»“æž„)
- [å®‰è£…](#å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†ç”¨æ³•](#è¯¦ç»†ç”¨æ³•)
- [æµ‹è¯•](#æµ‹è¯•)
- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)

---

## ðŸŽ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®žçŽ°äº†åŸºäºŽ**ç¨€ç–è‡ªç¼–ç å™¨ (SAE)** çš„å¯è§£é‡Šæ€§åˆ†æžæ¡†æž¶ï¼Œç”¨äºŽæŽ¢ç´¢ **Helix-mRNA** åŸºç¡€æ¨¡åž‹çš„å†…éƒ¨è¡¨ç¤ºã€‚é€šè¿‡å­¦ä¹ è¿‡å®Œå¤‡çš„ç¨€ç–ç‰¹å¾å­—å…¸ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

- ðŸ” **å‘çŽ°å¯è§£é‡Šçš„ç‰¹å¾**ï¼šä»Žæ¨¡åž‹æ¿€æ´»ä¸­æå–ç”Ÿç‰©å­¦ä¸Šæœ‰æ„ä¹‰çš„ç‰¹å¾
- ðŸ“Š **åˆ†æžå±‚çº§è¡¨ç¤º**ï¼šç†è§£ä¸åŒå±‚å¦‚ä½•ç¼–ç  RNA åºåˆ—ä¿¡æ¯
- ðŸ§¬ **ç”Ÿç‰©å­¦æ´žå¯Ÿ**ï¼šå°†å­¦åˆ°çš„ç‰¹å¾ä¸Žå·²çŸ¥çš„ç”Ÿç‰©å­¦æ¦‚å¿µå…³è”

### æ ¸å¿ƒç‰¹æ€§

âœ… **å®Œå…¨éžä¾µå…¥å¼**ï¼šä½¿ç”¨ PyTorch Hook æœºåˆ¶ï¼Œæ— éœ€ä¿®æ”¹åŽŸå§‹æ¨¡åž‹ä»£ç   
âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šSAE æ¨¡åž‹ã€è®­ç»ƒå™¨ã€Pipeline å®Œå…¨è§£è€¦  
âœ… **å¤šå±‚åˆ†æž**ï¼šæ”¯æŒåŒæ—¶åˆ†æžæ¨¡åž‹çš„å¤šä¸ªå±‚  
âœ… **çµæ´»é…ç½®**ï¼šå¯è‡ªå®šä¹‰è¿‡å®Œå¤‡æ¯”çŽ‡ã€ç¨€ç–æƒ©ç½šç³»æ•°ç­‰è¶…å‚æ•°  
âœ… **å®Œæ•´æµ‹è¯•**ï¼šåŒ…å«å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

---

## ðŸ§  æ ¸å¿ƒåŽŸç†

### ç¨€ç–è‡ªç¼–ç å™¨ (SAE)

SAE é€šè¿‡å­¦ä¹ è¿‡å®Œå¤‡çš„ç¨€ç–è¡¨ç¤ºæ¥å‘çŽ°æ•°æ®ä¸­çš„æ½œåœ¨ç‰¹å¾ï¼š

**é—®é¢˜å½¢å¼åŒ–**ï¼šä»Žè§‚æµ‹åˆ°çš„æ¿€æ´»å‘é‡é›†åˆ `{x_i}` ä¸­æ¢å¤ç‰¹å¾å­—å…¸ `{f_k}`ï¼Œä½¿å¾—æ¯ä¸ªæ¿€æ´»å¯ç”±å°‘æ•°ç‰¹å¾çš„çº¿æ€§ç»„åˆé‡æž„ã€‚

**æ¨¡åž‹æž¶æž„**ï¼š

```
ç¼–ç : c = ReLU(Wx + b)        # ç¨€ç–æ¿€æ´»
è§£ç : xÌ‚ = W^T c                # ç‰¹å¾é‡ç»„
```

**æŸå¤±å‡½æ•°**ï¼š

```
L(x) = ||x - xÌ‚||Â² + Î±||c||â‚
       â””â”€é‡æž„æŸå¤±â”€â”˜   â””â”€ç¨€ç–æƒ©ç½šâ”€â”˜
```

**å…³é”®è®¾è®¡**ï¼š
- **è¿‡å®Œå¤‡æ€§**ï¼š`d_hidden > d_in`ï¼ˆå…¸åž‹å€¼ï¼š4å€ï¼‰
- **ç¨€ç–çº¦æŸ**ï¼šL1 æƒ©ç½š + ReLU + è´Ÿåç½®
- **æƒé‡ç»‘å®š**ï¼šè§£ç å™¨ä½¿ç”¨ç¼–ç å™¨çš„è½¬ç½®
- **è¡Œå½’ä¸€åŒ–**ï¼š`||f_i||â‚‚ = 1`ï¼Œé˜²æ­¢èŒƒæ•°æ”¾å¤§

### å·¥ä½œæµç¨‹

```
RNA åºåˆ—
    â†“
[Helix mRNA æ¨¡åž‹]
    â†“
å¤šå±‚æ¿€æ´»æå– (Hook)
    â†“
{layer_0: activations, layer_1: activations, ...}
    â†“
è®­ç»ƒ SAE (æ¯å±‚ç‹¬ç«‹)
    â†“
ç‰¹å¾å­—å…¸ {fâ‚€, fâ‚, ..., f_n}
    â†“
å¯è§£é‡Šæ€§åˆ†æž
```

---

## ðŸ“ é¡¹ç›®ç»“æž„

```
SAE_Helix_mRNA/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ sparse_autoencoder.py      # SAE æ ¸å¿ƒæ¨¡åž‹
â”‚   â”‚   â”œâ”€â”€ activation_extractor.py    # æ¿€æ´»å€¼æå–å™¨ (Hook)
â”‚   â”‚   â”œâ”€â”€ sae_trainer.py             # SAE è®­ç»ƒå™¨
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ sae_pipeline.py            # å®Œæ•´åˆ†æž Pipeline
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_sparse_autoencoder.py     # SAE æ¨¡åž‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_activation_extractor.py   # æ¿€æ´»æå–æµ‹è¯•
â”‚   â”œâ”€â”€ test_sae_trainer.py            # è®­ç»ƒå™¨æµ‹è¯•
â”‚   â””â”€â”€ test_helix_integration.py      # Helix mRNA é›†æˆæµ‹è¯•
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ full_pipeline_example.py       # å®Œæ•´ç¤ºä¾‹
â”œâ”€â”€ outputs/                            # è¾“å‡ºç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ðŸš€ å®‰è£…

### 1. å…‹éš†ä»“åº“

```bash
cd /home/pan/Experiments/EXPs/2025_10_FM_explainability/SAE_Helix_mRNA
```

### 2. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install torch numpy tqdm

# Helix mRNA æ¨¡åž‹
pip install helical

# å¯é€‰ï¼šç”¨äºŽå¯è§†åŒ–
pip install matplotlib seaborn
```

### 3. éªŒè¯å®‰è£…

```bash
python test/test_sparse_autoencoder.py
```

---

## âš¡ å¿«é€Ÿå¼€å§‹

### æœ€ç®€ç¤ºä¾‹

```python
from helical.models.helix_mrna import HelixmRNA, HelixmRNAConfig
from src.pipeline import SAEAnalysisPipeline
import torch

# 1. åˆå§‹åŒ– Helix mRNA æ¨¡åž‹
device = "cuda" if torch.cuda.is_available() else "cpu"
helix_config = HelixmRNAConfig(batch_size=16, device=device)
helix_model = HelixmRNA(configurer=helix_config)

# 2. å‡†å¤‡ RNA åºåˆ—
sequences = ["EACUEGGG" * 20] * 1000
dataset = helix_model.process_data(sequences)

# 3. åˆ›å»º SAE Pipeline
pipeline = SAEAnalysisPipeline(
    helix_model=helix_model,
    expansion_factor=4,
    l1_coefficient=1e-3,
    device=device
)

# 4. è¿è¡Œå®Œæ•´åˆ†æž
results = pipeline.run_full_analysis(
    dataset=dataset,
    layer_filter=lambda name, m: 'mixer' in name.lower(),
    num_epochs=100,
    save_dir='./outputs/my_analysis'
)

# 5. æŸ¥çœ‹ç»“æžœ
for layer_name, analysis in results['feature_analyses'].items():
    print(f"{layer_name}: {analysis['n_features']} features")
```

---

## ðŸ“– è¯¦ç»†ç”¨æ³•

### 1. æå–æ¿€æ´»å€¼

```python
from src.model import ActivationExtractor

# åˆ›å»ºæå–å™¨
extractor = ActivationExtractor(helix_model.model)

# æ³¨å†Œè¦æå–çš„å±‚
extractor.register_hooks(
    layer_filter=lambda name, m: 'mixer' in name  # åªæå– mixer å±‚
)

# è¿è¡Œæ¨¡åž‹
embeddings = helix_model.get_embeddings(dataset)

# èŽ·å–æ¿€æ´»
activations = extractor.get_activations()
extractor.remove_hooks()
```

### 2. è®­ç»ƒ SAE

```python
from src.model import SparseAutoencoder, SAEConfig, SAETrainer

# é…ç½® SAE
config = SAEConfig(
    d_in=512,              # è¾“å…¥ç»´åº¦
    expansion_factor=4,    # è¿‡å®Œå¤‡æ¯”çŽ‡
    l1_coefficient=1e-3,   # L1 ç³»æ•°
    learning_rate=1e-3
)

# åˆ›å»ºæ¨¡åž‹
sae = SparseAutoencoder(config)

# è®­ç»ƒ
trainer = SAETrainer(sae, config, device='cuda')
history = trainer.train(
    activations=your_activations,
    num_epochs=100,
    batch_size=256,
    save_dir='./outputs/sae_checkpoints'
)
```

### 3. åˆ†æžç‰¹å¾

```python
# èŽ·å–ç‰¹å¾å­—å…¸
feature_dict = sae.get_feature_dictionary()  # (d_hidden, d_in)

# ç¼–ç æ–°çš„æ¿€æ´»
with torch.no_grad():
    features = sae.encode(new_activations)
    
# è®¡ç®—ç¨€ç–æ€§ç»Ÿè®¡
stats = sae.get_sparsity_stats(features)
print(f"L0 èŒƒæ•°: {stats['l0_norm']:.1f}")
print(f"ç¨€ç–åº¦: {stats['sparsity']:.2%}")
```

### 4. å¤šå±‚åˆ†æž

```python
from src.model import MultiLayerSAETrainer

# å‡†å¤‡å¤šå±‚æ¿€æ´»
layer_activations = {
    'layer_0': torch.randn(10000, 512),
    'layer_1': torch.randn(10000, 512),
    'layer_2': torch.randn(10000, 512),
}

# è®­ç»ƒæ‰€æœ‰å±‚
trainer = MultiLayerSAETrainer(
    layer_activations=layer_activations,
    expansion_factor=4,
    l1_coefficient=1e-3
)

all_histories = trainer.train_all(
    num_epochs=100,
    save_dir='./outputs/multi_layer'
)

# èŽ·å–ç‰¹å®šå±‚çš„ SAE
sae_layer0 = trainer.get_sae('layer_0')
```

---

## ðŸ§ª æµ‹è¯•

### è¿è¡Œæ‰€æœ‰æµ‹è¯•

```bash
# SAE æ¨¡åž‹æµ‹è¯•
python test/test_sparse_autoencoder.py

# æ¿€æ´»æå–æµ‹è¯•
python test/test_activation_extractor.py

# è®­ç»ƒå™¨æµ‹è¯•
python test/test_sae_trainer.py

# Helix mRNA é›†æˆæµ‹è¯•ï¼ˆéœ€è¦å®‰è£… helicalï¼‰
python test/test_helix_integration.py
```

### è¿è¡Œç¤ºä¾‹

```bash
python examples/full_pipeline_example.py
```

---

## ðŸ”§ é…ç½®å‚æ•°

### SAEConfig å‚æ•°è¯´æ˜Ž

| å‚æ•° | ç±»åž‹ | é»˜è®¤å€¼ | è¯´æ˜Ž |
|------|------|--------|------|
| `d_in` | int | - | è¾“å…¥ç»´åº¦ï¼ˆæ¿€æ´»å‘é‡ç»´åº¦ï¼‰ |
| `expansion_factor` | int | 4 | è¿‡å®Œå¤‡æ¯”çŽ‡ Rï¼ˆéšè—å±‚ = R Ã— è¾“å…¥å±‚ï¼‰ |
| `l1_coefficient` | float | 1e-3 | L1 ç¨€ç–æƒ©ç½šç³»æ•° Î± |
| `learning_rate` | float | 1e-3 | å­¦ä¹ çŽ‡ |
| `normalize_decoder` | bool | True | æ˜¯å¦å½’ä¸€åŒ–è§£ç å™¨æƒé‡ |
| `tied_weights` | bool | True | æ˜¯å¦ä½¿ç”¨æƒé‡ç»‘å®š |

### è®­ç»ƒå‚æ•°å»ºè®®

- **expansion_factor**: 2-8ï¼ˆè¶Šå¤§ç‰¹å¾è¶Šå¤šï¼Œä½†è®­ç»ƒè¶Šæ…¢ï¼‰
- **l1_coefficient**: 1e-4 åˆ° 1e-2ï¼ˆè¶Šå¤§è¶Šç¨€ç–ï¼‰
- **num_epochs**: 50-200ï¼ˆå–å†³äºŽæ•°æ®é‡ï¼‰
- **batch_size**: 128-512ï¼ˆå–å†³äºŽ GPU å†…å­˜ï¼‰

---

## ðŸ“Š è¾“å‡ºè¯´æ˜Ž

### è®­ç»ƒè¾“å‡º

```
outputs/
â””â”€â”€ my_analysis/
    â”œâ”€â”€ layer_name/
    â”‚   â”œâ”€â”€ best_model.pt              # æœ€ä½³æ¨¡åž‹æ£€æŸ¥ç‚¹
    â”‚   â”œâ”€â”€ final_model.pt             # æœ€ç»ˆæ¨¡åž‹
    â”‚   â””â”€â”€ training_history.json      # è®­ç»ƒåŽ†å²
    â””â”€â”€ analysis_results.pkl           # å®Œæ•´åˆ†æžç»“æžœ
```

### åˆ†æžç»“æžœ

`analysis_results.pkl` åŒ…å«ï¼š

```python
{
    'layer_activations_shapes': {...},  # æ¯å±‚æ¿€æ´»çš„å½¢çŠ¶
    'training_histories': {...},        # è®­ç»ƒåŽ†å²
    'feature_analyses': {
        'layer_name': {
            'n_features': int,                    # ç‰¹å¾æ•°é‡
            'feature_dim': int,                   # ç‰¹å¾ç»´åº¦
            'feature_activation_freq': ndarray,   # æ¿€æ´»é¢‘çŽ‡
            'feature_mean_activation': ndarray,   # å¹³å‡æ¿€æ´»
            'top_k_features': ndarray,            # Top-K ç‰¹å¾ç´¢å¼•
            'feature_dictionary': ndarray,        # ç‰¹å¾å­—å…¸çŸ©é˜µ
        }
    }
}
```

---

## ðŸ“š å‚è€ƒæ–‡çŒ®

1. **Towards Monosemanticity: Decomposing Language Models With Dictionary Learning**  
   Anthropic, 2023  
   https://arxiv.org/abs/2309.08600

2. **Helix-mRNA: A Foundation Model for mRNA Sequence Analysis**  
   Helical AI

3. **Sparse Autoencoders Find Highly Interpretable Features in Language Models**  
   Cunningham et al., 2023

---

## ðŸ¤ è´¡çŒ®

æ¬¢è¿Žæäº¤ Issue å’Œ Pull Requestï¼

---

## ðŸ“„ è®¸å¯è¯

MIT License

---

## ðŸ’¡ æç¤º

### å¸¸è§é—®é¢˜

**Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ expansion_factorï¼Ÿ**  
A: ä»Ž 4 å¼€å§‹å°è¯•ã€‚å¦‚æžœç‰¹å¾ä¸å¤Ÿä¸°å¯Œï¼Œå¢žåŠ åˆ° 8ï¼›å¦‚æžœè®­ç»ƒå¤ªæ…¢ï¼Œå‡å°‘åˆ° 2ã€‚

**Q: L1 ç³»æ•°åº”è¯¥è®¾ç½®å¤šå°‘ï¼Ÿ**  
A: ä»Ž 1e-3 å¼€å§‹ï¼Œè§‚å¯Ÿ L0 èŒƒæ•°ã€‚ç›®æ ‡æ˜¯æ¿€æ´» 5-10% çš„ç‰¹å¾ã€‚

**Q: è®­ç»ƒéœ€è¦å¤šå°‘æ•°æ®ï¼Ÿ**  
A: å»ºè®®è‡³å°‘ 10,000 ä¸ªæ¿€æ´»å‘é‡ã€‚æ•°æ®è¶Šå¤šï¼Œå­¦åˆ°çš„ç‰¹å¾è¶Šç¨³å®šã€‚

**Q: å¦‚ä½•è§£é‡Šå­¦åˆ°çš„ç‰¹å¾ï¼Ÿ**  
A: æŸ¥çœ‹ç‰¹å¾å­—å…¸å‘é‡ï¼Œåˆ†æžå“ªäº›è¾“å…¥ç»´åº¦æƒé‡æœ€å¤§ã€‚å¯ä»¥ä¸Žå·²çŸ¥çš„ç”Ÿç‰©å­¦æ¦‚å¿µå…³è”ã€‚

### æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨ GPU åŠ é€Ÿè®­ç»ƒ
- å¯¹äºŽå¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨ `max_samples` é™åˆ¶æ ·æœ¬æ•°
- å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆéœ€è¦ PyTorch >= 1.6ï¼‰

---

**Happy Exploring! ðŸš€**
